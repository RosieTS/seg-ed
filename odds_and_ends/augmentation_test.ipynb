{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' on line 146 (1829623210.py, line 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '(' on line 146\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, BCELoss\n",
    "from torch.optim import Adam, Optimizer\n",
    "\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as func\n",
    "import torchvision.utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def convert_target_pil_to_tensor(pil_img) -> Tensor:\n",
    "    \"\"\"Convert the target mask from pillow image to tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pil_img : PIL.Image\n",
    "        The segmentation mask as a pillow image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    target : Tensor\n",
    "        The segmentation mask as a (C, H, W) Tensor.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If:\n",
    "        grey[i, j] = 0, target[:, i, j] = [1, 0, 0, ...]\n",
    "        grey[i, j] = 1, target[:, i, j] = [0, 1, 0, ...]\n",
    "        grey[i, j] = 2, target[:, i, j] = [0, 0, 1, ...],\n",
    "\n",
    "        etc.\n",
    "\n",
    "    \"\"\"\n",
    "    grey = func.pil_to_tensor(pil_img).squeeze()\n",
    "    grey[grey == 255] = 21\n",
    "    num_classes = 22\n",
    "    target = torch.eye(num_classes)[grey.long()].permute(2, 0, 1).float()\n",
    "    return target\n",
    "\n",
    "'''\n",
    "randcrop_images = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(size=572,scale=(0.25,1.0)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "randcrop_targets = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(size=572,scale=(0.25,1.0)),\n",
    "        convert_target_pil_to_tensor, \n",
    "        # Resizing because images all different sizing. Could instead pad or make custom collate.\n",
    "        # Set to 572 x 572 to match original UNet paper\n",
    "        #transforms.Resize([572, 572]),\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "\n",
    "img_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # Resizing because images all different sizing. Could instead pad or make custom collate.\n",
    "        # Set to 572 x 572 to match original UNet paper\n",
    "        transforms.Resize([572, 572]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transforms = transforms.Compose(\n",
    "    # Set to 572 x 572 to match original UNet paper\n",
    "    [convert_target_pil_to_tensor, transforms.Resize([572, 572])]\n",
    ")\n",
    "\n",
    "\n",
    "def get_data_set_and_loader(img_set) -> Tuple[Dataset, DataLoader]:\n",
    "    \"\"\"Return a dataset and dataloader to use in training/validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : Namespace\n",
    "        Command-line arguments.\n",
    "    img_set : Image Set to use. \n",
    "        \"train\" or \"val\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_set : Dataset\n",
    "        The requested dataset.\n",
    "    data_loader : DataLoader\n",
    "        The requested dataloader.\n",
    "\n",
    "    \"\"\"\n",
    "    if img_set == 'train':\n",
    "        shuffle_img = True\n",
    "    elif img_set == 'val':\n",
    "        shuffle_img = False\n",
    "    else:\n",
    "        raise ValueError(f\"Image set option {img_set} is not acceptable.\")\n",
    "\n",
    "\n",
    "    data_set = VOCSegmentation(\n",
    "        \"data\",\n",
    "        #image_set=\"train\",\n",
    "        image_set=img_set,\n",
    "        download=False,\n",
    "        transform=img_transforms,\n",
    "        target_transform=target_transforms,\n",
    "    )\n",
    "\n",
    "    #data_set = data_subset(args, data_set)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        data_set, \n",
    "        batch_size=4, \n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    return data_set, data_loader\n",
    "\n",
    "dataset, loader = get_data_set_and_loader(\"train\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function for inline image display\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "augment_transforms = transforms.Compose(\n",
    "    # Set to 572 x 572 to match original UNet paper\n",
    "    [   \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(572, scale=(0.25, 1.0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def data_augmenter(images, targets):\n",
    "    \"\"\"Transform a batch of images and targets using a random resized crop\"\"\"\n",
    "    img_concat = torch.cat((images, targets), dim=1)\n",
    "    augmented = augment_transforms(img_concat)\n",
    "    images_new = augmented[:, :3, :, :]\n",
    "    targets_new = augmented[:, 3:, :, :]\n",
    "\n",
    "    return images_new, targets_new\n",
    "\n",
    "\n",
    "dataiter = iter(loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "images, labels = data_augmenter(images, labels)\n",
    "\n",
    "print(images.size())\n",
    "print(labels.size())\n",
    "\n",
    "#img_concat = torch.cat((images, labels), dim=1)\n",
    "\n",
    "#augmented = transforms.RandomResizedCrop(572, scale=(0.25, 1.0))(img_concat)\n",
    "\n",
    "#images_new = augmented[:, :3, :, :]\n",
    "#labels_new = augmented[:, 3:, :, :]\n",
    "\n",
    "# Create a grid from the images and show them\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=False)\n",
    "\n",
    "labels = labels.argmax(dim=1).unsqueeze(dim=1).float()\n",
    "labels.size()\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(labels)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('internship')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "555c3cc408bb6174d6f4ac59261404fbf4eabb1c64fd3e88880c2685b7aa3258"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
